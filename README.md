# ğŸ® Astray: Gesture-Controlled 3D Maze Game - research

A machine learning project that trains models to recognize hand gestures for controlling a 3D maze game. The project uses computer vision and machine learning to process hand landmarks and predict gestures.

## âœ¨ Features

- ğŸ¤š Hand gesture recognition using machine learning
- ğŸ§  Multiple model implementations (SVM, Random Forest, LightGBM)
- ğŸ“Š MLflow integration for experiment tracking
- ğŸ”„ Data preprocessing and normalization
- ğŸ“ˆ Model evaluation with confusion matrices
- âœ… Cross-validation and performance metrics

## ğŸ† Model Performance

Among the three implemented models (SVM, Random Forest, and LightGBM), LightGBM demonstrated the best performance in terms of:
- ğŸ¯ Higher accuracy on both validation and test sets
- ğŸ“ˆ Better F1 scores
- âš¡ Faster training and inference times
- ğŸ¯ More robust predictions across different gesture classes

<img src="./assets/models.png" alt="Maze Demo" width="500"/>
<img src="./assets/lgbm.png" alt="Maze Demo" width="500"/>
<img src="./assets/rf.png" alt="Maze Demo" width="500"/>
<img src="./assets/svm.png" alt="Maze Demo" width="500"/>


## ğŸ“‹ Requirements

- ğŸ Python 3.x
- ğŸ“¦ Dependencies listed in `requirements.txt`:
  - mlflow >= 2.12.0
  - scikit-learn >= 1.4.2
  - lightgbm >= 4.1.0
  - opencv-python-headless >= 4.12.0
  - pandas >= 2.2.2
  - numpy >= 1.26.4
  - joblib >= 1.4.0

## ğŸš€ Installation

1. Clone the repository:
```bash
git clone https://github.com/yourusername/Astray-Gesture-controlled-3D-Maze-Game.git
cd Astray-Gesture-controlled-3D-Maze-Game
```

2. Install dependencies:
```bash
pip install -r requirements.txt
```

## ğŸ’» Usage

### ğŸ¯ Training Models

To train the gesture recognition models:

```bash
python train.py --data data/hand_landmarks_data.csv --models svm rf lgbm --out_dir models
```

Arguments:
- `--data`: Path to the hand landmarks dataset (default: "data/hand_landmarks_data.csv")
- `--models`: List of models to train (default: ["svm", "rf", "lgbm"])
- `--out_dir`: Directory to save trained models (default: "models")

### ğŸ”§ Data Preprocessing

The training script includes preprocessing steps:
- ğŸ§¹ Drops NaN values
- ğŸ“ Anchors all landmarks at the wrist
- ğŸ“ Normalizes coordinates using wrist-to-mid-finger-tip distance
- ğŸ“Š Splits data into train/validation/test sets (80/10/10)

### ğŸ“Š Model Evaluation

The training process evaluates models using:
- ğŸ¯ Accuracy score
- ğŸ“ˆ F1 score
- ğŸ“Š Confusion matrices (saved as PNG files)

Results are logged using MLflow for experiment tracking.

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ ğŸ“‚ data/               # Dataset directory
â”œâ”€â”€ ğŸ“‚ models/            # Trained model outputs
â”œâ”€â”€ ğŸ“‚ mlruns/           # MLflow experiment tracking
â”œâ”€â”€ ğŸ“„ train.py          # Training script
â””â”€â”€ ğŸ“„ requirements.txt  # Project dependencies
```
